---
title: "Efficient continual pre-training LLMs for financial domains"
date: 2024-03-28T16:08:19+00:00
link: https://aws.amazon.com/blogs/machine-learning/efficient-continual-pre-training-llms-for-financial-domains/
showShare: false
showReadTime: false
thumbnail: images/ai.png
tags: ["aws.amazon.com/blogs/machine-learning"]
---
Large language models (LLMs) are generally trained on large publicly available datasets that are domain agnostic. For example, Meta’s Llama models are trained on datasets such as CommonCrawl, C4, Wikipedia, and ArXiv. These datasets encompass a broad range of topics and domains. Although the resulting models yield amazingly good results for general tasks, such as […]

- Link to article: https://aws.amazon.com/blogs/machine-learning/efficient-continual-pre-training-llms-for-financial-domains/