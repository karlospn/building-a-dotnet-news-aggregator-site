---
title: "Reducing hallucinations in LLM agents with a verified semantic cache using Amazon Bedrock Knowledge Bases"
date: 2025-02-21T16:36:30+00:00
link: https://aws.amazon.com/blogs/machine-learning/reducing-hallucinations-in-llm-agents-with-a-verified-semantic-cache-using-amazon-bedrock-knowledge-bases/
showShare: false
showReadTime: false
thumbnail: images/aws.png
tags: ["aws.amazon.com/blogs/machine-learning"]
---
This post introduces a solution to reduce hallucinations in Large Language Models (LLMs) by implementing a verified semantic cache using Amazon Bedrock Knowledge Bases, which checks if user questions match curated and verified responses before generating new answers. The solution combines the flexibility of LLMs with reliable, verified answers to improve response accuracy, reduce latency, and lower costs while preventing potential misinformation in critical domains such as healthcare, finance, and legal services.

- Link to article: https://aws.amazon.com/blogs/machine-learning/reducing-hallucinations-in-llm-agents-with-a-verified-semantic-cache-using-amazon-bedrock-knowledge-bases/