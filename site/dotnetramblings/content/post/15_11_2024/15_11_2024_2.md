---
title: "Fine-tune multimodal models for vision and text use cases on Amazon SageMaker JumpStart"
date: 2024-11-15T16:53:23+00:00
link: https://aws.amazon.com/blogs/machine-learning/fine-tune-multimodal-models-for-vision-and-text-use-cases-on-amazon-sagemaker-jumpstart/
showShare: false
showReadTime: false
thumbnail: images/aws.png
tags: ["aws.amazon.com/blogs/machine-learning"]
---
In this post, we showcase how to fine-tune a text and vision model, such as Meta Llama 3.2, to better perform at visual question answering tasks. The Meta Llama 3.2 Vision Instruct models demonstrated impressive performance on the challenging DocVQA benchmark for visual question answering. By using the power of Amazon SageMaker JumpStart, we demonstrate the process of adapting these generative AI models to excel at understanding and responding to natural language questions about images.

- Link to article: https://aws.amazon.com/blogs/machine-learning/fine-tune-multimodal-models-for-vision-and-text-use-cases-on-amazon-sagemaker-jumpstart/